{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhrubaAdhikary/GEN_AI_DEMO/blob/master/Memory_in_RAGs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Zuw_N6Xu8k"
      },
      "source": [
        "# RAG with Memory: Comprehensive Architecture Design\n",
        "\n",
        "## üéØ Overview\n",
        "This notebook demonstrates a **Retrieval-Augmented Generation (RAG) system with Memory Management** using LangChain and OpenAI. The system combines the power of conversational memory with vector-based retrieval to create context-aware AI applications.\n",
        "\n",
        "## üèóÔ∏è Architecture Components\n",
        "\n",
        "### 1. **Conversational Memory Layer**\n",
        "- **In-Memory Chat History**: Stores conversation context for continuity\n",
        "- **Session Management**: Maintains separate conversation threads per user\n",
        "- **Message Persistence**: Retains both user inputs and AI responses\n",
        "\n",
        "### 2. **Memory Management Strategies**\n",
        "\n",
        "#### A. Full Memory (Unlimited Context)\n",
        "```\n",
        "User Message 1 ‚Üí AI Response 1\n",
        "User Message 2 ‚Üí AI Response 2\n",
        "    ...\n",
        "User Message N ‚Üí AI Response N\n",
        "[All messages retained]\n",
        "```\n",
        "**Pros**: Complete context awareness  \n",
        "**Cons**: Token limits, increased costs\n",
        "\n",
        "#### B. Windowed Memory (Last K Messages)\n",
        "```\n",
        "[Older messages discarded]\n",
        "User Message (N-K+1) ‚Üí AI Response (N-K+1)\n",
        "    ...\n",
        "User Message N ‚Üí AI Response N\n",
        "[Only last K messages retained]\n",
        "```\n",
        "**Pros**: Controlled token usage, scalable  \n",
        "**Cons**: Loss of older context\n",
        "\n",
        "#### C. Summary-Based Memory\n",
        "```\n",
        "[Message 1...N] ‚Üí Compressed Summary\n",
        "Recent Messages (Last K)\n",
        "[Combined for context]\n",
        "```\n",
        "**Pros**: Retains historical essence, efficient  \n",
        "**Cons**: Information loss, summary generation cost\n",
        "\n",
        "### 3. **Retrieval-Augmented Generation (RAG)**\n",
        "- **Vector Embeddings**: Converts text into numerical representations\n",
        "- **FAISS Vector Store**: Fast similarity search for relevant information\n",
        "- **Semantic Retrieval**: Finds contextually relevant documents/facts\n",
        "- **Context Injection**: Augments prompts with retrieved information\n",
        "\n",
        "## üîÑ System Workflow\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  User Query     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "         ‚îÇ                                  ‚îÇ\n",
        "         ‚ñº                                  ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Memory Retrieval   ‚îÇ          ‚îÇ  Vector Search   ‚îÇ\n",
        "‚îÇ  (Chat History)     ‚îÇ          ‚îÇ  (RAG System)    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "           ‚îÇ                               ‚îÇ\n",
        "           ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
        "           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  LLM (GPT)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           ‚îÇ\n",
        "                           ‚ñº\n",
        "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                   ‚îÇ  AI Response  ‚îÇ\n",
        "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## üéì Learning Objectives\n",
        "\n",
        "1. **Environment Setup**: Configure API keys and import dependencies\n",
        "2. **Basic Memory**: Implement simple conversational memory\n",
        "3. **Windowed Memory**: Manage context with sliding windows\n",
        "4. **Memory Summarization**: Compress conversation history\n",
        "5. **Vector Storage**: Create and query FAISS vector stores\n",
        "6. **RAG Integration**: Combine retrieval with generation\n",
        "\n",
        "## üõ†Ô∏è Technologies Used\n",
        "\n",
        "- **LangChain**: Framework for LLM applications\n",
        "- **OpenAI GPT-3.5**: Language model for generation\n",
        "- **FAISS**: Facebook AI Similarity Search for vector retrieval\n",
        "- **OpenAI Embeddings**: Text-to-vector conversion\n",
        "- **Python**: Core implementation language\n",
        "\n",
        "---\n",
        "\n",
        "Let's explore each component step by step! üëá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87E-VV8Xu8o"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rauDkHaKXu8p"
      },
      "source": [
        "Where the Memory Lives\n",
        "As shown in the architecture diagram:\n",
        "\n",
        "Short-Term Memory: Acts as the \"immediate context,\" feeding the most recent chat history into the LLM.\n",
        "\n",
        "Semantic Memory: The massive library of facts stored in your Vector DB (like Qdrant or Pinecone).\n",
        "\n",
        "Episodic Memory: This is where past user preferences and historical summaries are stored, allowing for continuity over months or years.\n",
        "\n",
        "All three layers funnel into the LLM Brain / Orchestrator to generate a grounded, context-aware response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTIQ8TrjXu8p"
      },
      "source": [
        "## 1. Environment Setup and API Configuration\n",
        "\n",
        "This section initializes the OpenAI API key from Google Colab's secure storage (userdata). The API key is essential for:\n",
        "- Authenticating with OpenAI services\n",
        "- Accessing GPT models for text generation\n",
        "- Using OpenAI's embedding models for vector representations\n",
        "\n",
        "**Key Concepts:**\n",
        "- **API Key Security**: Stored securely in Colab's userdata, never hardcoded\n",
        "- **Environment Variables**: Standard practice for managing credentials\n",
        "- **Assertion Check**: Validates the key is successfully loaded before proceeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXArYu27z_q9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "assert os.environ[\"OPENAI_API_KEY\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp12a0_MXu8q"
      },
      "source": [
        "## 2. Core Library Imports\n",
        "\n",
        "Importing essential LangChain components for building our RAG system with memory:\n",
        "\n",
        "**Components Breakdown:**\n",
        "- **`ChatOpenAI`**: Interface to OpenAI's chat models (GPT-3.5/4)\n",
        "- **`OpenAIEmbeddings`**: Converts text to vector embeddings for similarity search\n",
        "- **`ChatPromptTemplate`**: Structures prompts with system messages, history, and user input\n",
        "- **`RunnableWithMessageHistory`**: Wrapper that adds conversation memory to any chain\n",
        "- **`FAISS`**: High-performance vector database for similarity search (Facebook AI Similarity Search)\n",
        "\n",
        "These components form the foundation of our memory-enabled RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lASwHNVC2qlh"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "# from langchain_core.chat_history import ChatMessageHistory\n",
        "from langchain_community.vectorstores import FAISS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLj57NjXu8s"
      },
      "source": [
        "## 3. In-Memory Chat History Implementation\n",
        "\n",
        "Importing the **`InMemoryChatMessageHistory`** class, which provides a simple storage mechanism for conversation messages.\n",
        "\n",
        "**Why In-Memory Storage?**\n",
        "- ‚úÖ Fast access - no disk I/O or database queries\n",
        "- ‚úÖ Simple implementation for prototyping and demos\n",
        "- ‚úÖ Automatic message ordering and retrieval\n",
        "- ‚ö†Ô∏è Data lost when process terminates (not persistent)\n",
        "- ‚ö†Ô∏è Limited to single-process applications\n",
        "\n",
        "**Use Cases:**\n",
        "- Development and testing\n",
        "- Short-lived conversational sessions\n",
        "- Proof-of-concept applications\n",
        "\n",
        "For production systems, consider persistent storage like Redis, PostgreSQL, or MongoDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Af1K7529gf"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJYjccCVXu8t"
      },
      "source": [
        "## 4. Initialize the Language Model\n",
        "\n",
        "Creating an instance of **GPT-3.5-Turbo** with specific configuration:\n",
        "\n",
        "**Parameters:**\n",
        "- **`model=\"gpt-3.5-turbo\"`**: Uses OpenAI's GPT-3.5 model\n",
        "  - Fast response times\n",
        "  - Cost-effective for most use cases\n",
        "  - Good balance between performance and price\n",
        "  \n",
        "- **`temperature=0`**: Controls randomness in responses\n",
        "  - `0` = Deterministic, consistent outputs\n",
        "  - Ideal for factual queries and consistent behavior\n",
        "  - Higher values (0.7-1.0) encourage creative, varied responses\n",
        "\n",
        "This LLM instance will be the core reasoning engine for our conversational AI system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AODGyppn2so_"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkVmhIDbXu8u"
      },
      "source": [
        "## 5. Session Management with Memory Store\n",
        "\n",
        "Implementing a **session-based memory management system** for handling multiple concurrent conversations:\n",
        "\n",
        "**Architecture:**\n",
        "```python\n",
        "store = {\n",
        "    \"session_1\": InMemoryChatMessageHistory([msg1, msg2, ...]),\n",
        "    \"session_2\": InMemoryChatMessageHistory([msg1, msg2, ...]),\n",
        "    ...\n",
        "}\n",
        "```\n",
        "\n",
        "**Key Functions:**\n",
        "- **`get_session_history(session_id)`**: Retrieves or creates a conversation history\n",
        "  - If session exists ‚Üí returns existing history\n",
        "  - If new session ‚Üí creates new `InMemoryChatMessageHistory` instance\n",
        "  \n",
        "**Benefits:**\n",
        "- üë• **Multi-User Support**: Each user gets isolated conversation context\n",
        "- üîí **Session Isolation**: Prevents context bleeding between conversations\n",
        "- üéØ **Lazy Initialization**: Sessions created only when needed\n",
        "- üìä **Scalable**: Can handle multiple concurrent conversations\n",
        "\n",
        "**Example Use Cases:**\n",
        "- Chat applications with multiple users\n",
        "- A/B testing different conversation flows\n",
        "- Parallel conversation experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPWvklIo2xdq"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjAeTxPCXu8u"
      },
      "source": [
        "## 6. Building the Conversational Chain with Full Memory\n",
        "\n",
        "Constructing a complete **memory-enabled conversational AI pipeline**:\n",
        "\n",
        "**Components:**\n",
        "\n",
        "1. **Prompt Template Structure:**\n",
        "   ```\n",
        "   System Message: \"You are a helpful assistant.\"\n",
        "   ‚Üì\n",
        "   Conversation History: {history} (all previous messages)\n",
        "   ‚Üì\n",
        "   Current User Input: {input}\n",
        "   ```\n",
        "\n",
        "2. **Chain Construction (`prompt | llm`):**\n",
        "   - Pipes the formatted prompt into the language model\n",
        "   - LangChain Expression Language (LCEL) for composability\n",
        "\n",
        "3. **RunnableWithMessageHistory Wrapper:**\n",
        "   - **`chain`**: The base prompt + LLM pipeline\n",
        "   - **`get_session_history`**: Function to retrieve/store messages\n",
        "   - **`input_messages_key=\"input\"`**: Maps user input to prompt variable\n",
        "   - **`history_messages_key=\"history\"`**: Maps stored messages to prompt variable\n",
        "\n",
        "**Memory Behavior:**\n",
        "- ‚úÖ **Full Context**: All messages from session start retained\n",
        "- ‚úÖ **Automatic Persistence**: Conversations saved to `store` after each interaction\n",
        "- ‚úÖ **Context Awareness**: Model remembers all previous exchanges\n",
        "\n",
        "**Flow Diagram:**\n",
        "```\n",
        "User Input ‚Üí Retrieve History ‚Üí Format Prompt ‚Üí LLM ‚Üí Save Response ‚Üí Return\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HC3209n21oe"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"placeholder\", \"{history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "chain_with_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnelG1bgXu8u"
      },
      "source": [
        "## 7. Testing Full Memory - Conversational Persistence\n",
        "\n",
        "Demonstrating **full memory retention** across multiple conversation turns:\n",
        "\n",
        "**Test Scenario:**\n",
        "1. **First Message**: \"My name is Dhruba\"\n",
        "2. **Second Message**: \"I work on graph neural networks\"\n",
        "\n",
        "**Expected Behavior:**\n",
        "- Both messages stored in `store[\"demo\"]`\n",
        "- LLM has access to all previous context\n",
        "- Can answer questions like \"What's my name?\" or \"What do I work on?\"\n",
        "\n",
        "**What We're Observing:**\n",
        "- **`store[\"demo\"].messages`**: Displays the complete conversation history\n",
        "  - User messages (HumanMessage)\n",
        "  - AI responses (AIMessage)\n",
        "  - Preserves chronological order\n",
        "\n",
        "**Memory Pattern:**\n",
        "```\n",
        "Turn 1: User ‚Üí \"My name is Dhruba\" ‚Üí AI response ‚Üí [saved]\n",
        "Turn 2: User ‚Üí \"I work on GNNs\" ‚Üí AI response (knows your name) ‚Üí [saved]\n",
        "...\n",
        "[All messages accumulated]\n",
        "```\n",
        "\n",
        "This is useful when you need **complete context** but can become expensive with long conversations due to token limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yovoKR483OTN",
        "outputId": "83f1232f-8e64-42d6-97e3-03aeccdfa2af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='My name is Dhruba', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Nice to meet you, Dhruba! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 23, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4PUNy7E9xWv7NAGcxP3BFApjLo9q', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18dd-0096-7c71-8340-ec5731f51470-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 23, 'output_tokens': 16, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='I work on graph neural networks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's great! Graph neural networks (GNNs) are a fascinating area of research with many applications in various fields, such as social network analysis, recommendation systems, and molecular chemistry. What specific aspects of GNNs are you working on, or do you have any questions or topics you'd like to discuss?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 53, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4PUO5i3ig3jeAldQfCsTH32TW9OX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18dd-05c6-7621-98f1-19cfd0b4e537-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 53, 'output_tokens': 62, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_memory.invoke(\n",
        "    {\"input\": \"My name is Dhruba\"},\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
        ")\n",
        "\n",
        "chain_with_memory.invoke(\n",
        "    {\"input\": \"I work on graph neural networks\"},\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
        ")\n",
        "\n",
        "store[\"demo\"].messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fJbMMkXu8v"
      },
      "source": [
        "## 8. Windowed Memory Implementation - Token Optimization\n",
        "\n",
        "Introducing **sliding window memory** to manage long conversations efficiently:\n",
        "\n",
        "**The Problem with Full Memory:**\n",
        "- Token costs grow linearly with conversation length\n",
        "- Risk hitting model context limits (4K, 16K, 128K tokens)\n",
        "- Unnecessary older context may dilute recent, relevant information\n",
        "\n",
        "**The Solution: `get_windowed_history(session_id, k=4)`**\n",
        "\n",
        "**How It Works:**\n",
        "```python\n",
        "Original: [Msg1, Msg2, Msg3, Msg4, Msg5, Msg6, Msg7, Msg8]\n",
        "                                    ‚Üì\n",
        "Window (k=4): [Msg5, Msg6, Msg7, Msg8]  # Last 4 messages only\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- **`hist.messages = hist.messages[-k:]`**: Python slice keeps last K messages\n",
        "- **Default k=4**: Retains 2 conversation turns (2 user + 2 AI messages)\n",
        "- **Dynamic Pruning**: Automatically discards older messages\n",
        "\n",
        "**Trade-offs:**\n",
        "- ‚úÖ **Fixed token cost**: Predictable, controlled context size\n",
        "- ‚úÖ **Scalable**: Works for indefinitely long conversations\n",
        "- ‚úÖ **Recent context focus**: Emphasizes latest exchanges\n",
        "- ‚ö†Ô∏è **Context loss**: Older information forgotten\n",
        "\n",
        "**When to Use:**\n",
        "- Long-running chat sessions\n",
        "- Cost-sensitive applications\n",
        "- When recent context is most important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HBhYEnT3P_O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "def get_windowed_history(session_id: str, k=4):\n",
        "    hist = get_session_history(session_id)\n",
        "    hist.messages = hist.messages[-k:]\n",
        "    return hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zliUyL_iXu8v"
      },
      "source": [
        "## 9. Creating Windowed Memory Chain\n",
        "\n",
        "Building a **new conversational chain** with the windowed memory function:\n",
        "\n",
        "**Critical Difference:**\n",
        "```python\n",
        "# Full Memory\n",
        "chain_with_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,  # ‚Üê Returns ALL messages\n",
        "    ...\n",
        ")\n",
        "\n",
        "# Windowed Memory\n",
        "chain_with_window_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_windowed_history,  # ‚Üê Returns LAST K messages only\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "**Configuration:**\n",
        "- Same prompt template (system + history + input)\n",
        "- Same LLM (GPT-3.5-turbo)\n",
        "- **Different memory retrieval function**: `get_windowed_history` instead of `get_session_history`\n",
        "\n",
        "**Impact:**\n",
        "- Model only sees the last 4 messages (k=4)\n",
        "- Recent conversational context preserved\n",
        "- Older messages automatically pruned\n",
        "- Consistent token usage across long conversations\n",
        "\n",
        "This approach is particularly useful for:\n",
        "- Customer support chatbots (focus on current issue)\n",
        "- Task-oriented conversations\n",
        "- Applications with strict latency requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ0DU6m53xkG"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"placeholder\", \"{history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "chain_with_window_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_windowed_history,      # üî¥ IMPORTANT\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q0exmMAXu8w"
      },
      "source": [
        "## 10. Testing Windowed Memory - Multiple Conversation Turns\n",
        "\n",
        "Running **4 consecutive messages** to demonstrate windowed memory in action:\n",
        "\n",
        "**Conversation Sequence:**\n",
        "1. \"I live in Bangalore\" ‚Üí üèôÔ∏è Location information\n",
        "2. \"I work at AstraZeneca\" ‚Üí üíº Employment details\n",
        "3. \"I build agentic AI systems\" ‚Üí ü§ñ Work focus\n",
        "4. \"I focus on A2A architectures\" ‚Üí üèóÔ∏è Technical specialization\n",
        "\n",
        "**Expected Memory Behavior:**\n",
        "- After message 1: [Msg1]\n",
        "- After message 2: [Msg1, Msg2]\n",
        "- After message 3: [Msg1, Msg2, Msg3]\n",
        "- After message 4: [Msg1, Msg2, Msg3, Msg4] ‚úì (exactly k=4 messages)\n",
        "- If we add message 5: [Msg2, Msg3, Msg4, Msg5] (Msg1 dropped!)\n",
        "\n",
        "**What to Observe:**\n",
        "- Window size caps at 4 messages\n",
        "- Oldest messages automatically pruned beyond window\n",
        "- Model maintains coherent recent context\n",
        "\n",
        "**Testing Questions (try after running):**\n",
        "- ‚úÖ \"Where do I work?\" ‚Üí Should answer (within window)\n",
        "- ‚úÖ \"What do I build?\" ‚Üí Should answer (within window)\n",
        "- ‚ùå \"Where do I live?\" ‚Üí May forget after 3+ more messages (outside window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEbRm8xh3xgd",
        "outputId": "912d9568-b0f1-4f9d-b2ef-4fcaa00289c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"That's great to hear! A2A architectures, which stands for Application-to-Application architectures, are crucial for enabling communication and interaction between different software applications. It's a key aspect of building complex systems that can work together seamlessly. If you have any questions or need assistance related to A2A architectures or any other topic, feel free to ask. I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 180, 'total_tokens': 257, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4PX0f90QRKhKtfCnpPP0o8SfU6Z8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18df-7f98-77d2-8403-5cb96c5f5f39-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 180, 'output_tokens': 77, 'total_tokens': 257, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_window_memory.invoke(\n",
        "    {\"input\": \"I live in Bangalore\"},\n",
        "    config={\"configurable\": {\"session_id\": \"win\"}}\n",
        ")\n",
        "\n",
        "chain_with_window_memory.invoke(\n",
        "    {\"input\": \"I work at AstraZeneca\"},\n",
        "    config={\"configurable\": {\"session_id\": \"win\"}}\n",
        ")\n",
        "\n",
        "chain_with_window_memory.invoke(\n",
        "    {\"input\": \"I build agentic AI systems\"},\n",
        "    config={\"configurable\": {\"session_id\": \"win\"}}\n",
        ")\n",
        "\n",
        "chain_with_window_memory.invoke(\n",
        "    {\"input\": \"I focus on A2A architectures\"},\n",
        "    config={\"configurable\": {\"session_id\": \"win\"}}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkJ2HMqQXu8w"
      },
      "source": [
        "## 11. Inspecting Windowed Memory State\n",
        "\n",
        "Examining the **actual message history** stored in the windowed session:\n",
        "\n",
        "**What This Shows:**\n",
        "- Direct access to `store[\"win\"].messages`\n",
        "- Reveals which messages are retained in memory\n",
        "- Confirms the window size limitation\n",
        "\n",
        "**Expected Output:**\n",
        "```python\n",
        "[\n",
        "    HumanMessage(content=\"I live in Bangalore\"),\n",
        "    AIMessage(content=\"...\"),\n",
        "    HumanMessage(content=\"I work at AstraZeneca\"),\n",
        "    AIMessage(content=\"...\"),\n",
        "    HumanMessage(content=\"I build agentic AI systems\"),\n",
        "    AIMessage(content=\"...\"),\n",
        "    HumanMessage(content=\"I focus on A2A architectures\"),\n",
        "    AIMessage(content=\"...\")\n",
        "]\n",
        "# Total: 8 messages (4 user + 4 AI)\n",
        "```\n",
        "\n",
        "**Note:** With k=4 for `get_windowed_history`, we're tracking the last **4 messages total** (not 4 turns). If each turn = 2 messages (user + AI), we retain **2 full conversation turns**.\n",
        "\n",
        "**Debugging Tip:**\n",
        "Inspecting the message store is crucial for:\n",
        "- Verifying memory strategy implementation\n",
        "- Debugging context issues\n",
        "- Understanding what information is available to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccjenM973xYJ",
        "outputId": "6a67ec30-9722-4e63-e341-ad3a9e0544cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='I work at AstraZeneca', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's wonderful! AstraZeneca is a global biopharmaceutical company known for its innovative medicines and contributions to healthcare. If you have any questions or need assistance related to your work at AstraZeneca or anything else, feel free to ask. I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 72, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4PWyGvqLjqNgt0KcwvasD04CuQWW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18df-74d9-7e52-9795-21944988327a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 72, 'output_tokens': 58, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='I build agentic AI systems', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's fascinating! Building agentic AI systems involves creating intelligent systems that can act autonomously and make decisions on their own. It's a cutting-edge field with a lot of potential for innovation and impact. If you have any specific questions or need assistance related to building agentic AI systems, feel free to ask. I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 144, 'total_tokens': 213, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4PWzYjl5mzg8ORl1eHyfuVIs8ngu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18df-7a12-7a23-88f9-8e797446e589-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 144, 'output_tokens': 69, 'total_tokens': 213, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " HumanMessage(content='I focus on A2A architectures', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's great to hear! A2A architectures, which stands for Application-to-Application architectures, are crucial for enabling communication and interaction between different software applications. It's a key aspect of building complex systems that can work together seamlessly. If you have any questions or need assistance related to A2A architectures or any other topic, feel free to ask. I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 180, 'total_tokens': 257, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4PX0f90QRKhKtfCnpPP0o8SfU6Z8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18df-7f98-77d2-8403-5cb96c5f5f39-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 180, 'output_tokens': 77, 'total_tokens': 257, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "store[\"win\"].messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-hKkIjXu8w"
      },
      "source": [
        "## 12. Memory Summarization - Compressing Conversation History\n",
        "\n",
        "Implementing **summarization-based memory** to preserve historical context efficiently:\n",
        "\n",
        "**The Strategy:**\n",
        "Instead of keeping all messages or just recent ones, **summarize** old conversations into compressed form:\n",
        "```\n",
        "[Old Messages] ‚Üí Summary ‚Üí [Recent Messages + Summary] ‚Üí LLM\n",
        "```\n",
        "\n",
        "**Implementation Steps:**\n",
        "\n",
        "1. **Summary Prompt Template:**\n",
        "   - System instruction: \"Summarize the conversation briefly.\"\n",
        "   - Input: Full conversation text\n",
        "\n",
        "2. **Summary Chain:**\n",
        "   - Pipes the prompt into the LLM\n",
        "   - Generates concise summary of key points\n",
        "\n",
        "3. **Conversation Text Preparation:**\n",
        "   - Extracts all messages from session\n",
        "   - Joins them into single text string\n",
        "   - Preserves conversational flow\n",
        "\n",
        "**Benefits:**\n",
        "- üì¶ **Compression**: Reduce 1000 tokens to 100 tokens\n",
        "- üß† **Context Preservation**: Retain essential information\n",
        "- üí∞ **Cost Efficiency**: Lower token usage than full history\n",
        "- üéØ **Semantic Retention**: Keep meaning, discard verbosity\n",
        "\n",
        "**Use Cases:**\n",
        "- Long-term memory systems\n",
        "- Multi-session conversations\n",
        "- Knowledge base construction from chats\n",
        "\n",
        "The resulting summary can be prepended to recent messages for hybrid memory approach!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9R2NYdu3Tbw",
        "outputId": "a3fb10d6-a54a-4be5-bc18-13ce501100d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"The user's name is Dhruba, and they work on graph neural networks (GNNs). GNNs are a fascinating area of research with applications in various fields. The conversation may involve discussing specific aspects of GNNs or addressing any questions or topics related to this area of research.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 112, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4PVNkgaDINqUEx3hjDPQj7e1Zb5p', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c18dd-f41c-7cc0-bd0e-3452ed7a4600-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 112, 'output_tokens': 60, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Summarize the conversation briefly.\"),\n",
        "    (\"human\", \"{conversation}\")\n",
        "])\n",
        "\n",
        "summary_chain = summary_prompt | llm\n",
        "\n",
        "conversation_text = \"\\n\".join(\n",
        "    m.content for m in store[\"demo\"].messages\n",
        ")\n",
        "\n",
        "summary = summary_chain.invoke(\n",
        "    {\"conversation\": conversation_text}\n",
        ")\n",
        "\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbOi0SAXu8x"
      },
      "source": [
        "## 13. RAG Implementation - Vector Store and Semantic Search\n",
        "\n",
        "Introducing **Retrieval-Augmented Generation (RAG)** with vector embeddings:\n",
        "\n",
        "**RAG Architecture:**\n",
        "```\n",
        "Knowledge Base ‚Üí Embeddings ‚Üí Vector Store ‚Üí Similarity Search ‚Üí Context ‚Üí LLM\n",
        "```\n",
        "\n",
        "**Components:**\n",
        "\n",
        "1. **OpenAI Embeddings:**\n",
        "   - Converts text into 1536-dimensional vectors\n",
        "   - Captures semantic meaning mathematically\n",
        "   - Similar meanings = similar vectors\n",
        "\n",
        "2. **FAISS Vector Store:**\n",
        "   - Facebook AI Similarity Search\n",
        "   - Efficient nearest-neighbor search\n",
        "   - Indexes vectors for fast retrieval\n",
        "\n",
        "3. **Knowledge Base:**\n",
        "   ```python\n",
        "   texts = [\n",
        "       \"User works on graph neural networks\",\n",
        "       \"User builds agentic AI systems\",\n",
        "       \"User is a senior data scientist\"\n",
        "   ]\n",
        "   ```\n",
        "   These facts are embedded and stored for retrieval\n",
        "\n",
        "4. **Similarity Search:**\n",
        "   - Query: \"What kind of systems does the user build?\"\n",
        "   - Returns: \"User builds agentic AI systems\" (k=1, top match)\n",
        "   - Based on cosine similarity between query and document vectors\n",
        "\n",
        "**Why RAG?**\n",
        "- üìö **External Knowledge**: Access information beyond training data\n",
        "- üéØ **Relevant Context**: Retrieve only pertinent information\n",
        "- ‚úÖ **Factual Accuracy**: Ground responses in specific documents\n",
        "- üîÑ **Dynamic Updates**: Add new knowledge without retraining\n",
        "\n",
        "**Combined with Memory:**\n",
        "- Memory: Tracks conversation flow\n",
        "- RAG: Provides factual knowledge\n",
        "- Together: Context-aware + knowledge-grounded AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7n3gVk773fNn",
        "outputId": "5f05fea5-02ee-49e7-8dcb-d084752a7de6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'User builds agentic AI systems'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "texts = [\n",
        "    \"User works on graph neural networks\",\n",
        "    \"User builds agentic AI systems\",\n",
        "    \"User is a senior data scientist\"\n",
        "]\n",
        "\n",
        "vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    \"What kind of systems does the user build?\", k=1\n",
        ")[0].page_content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZYpiKW2Xu8x"
      },
      "source": [
        "## 14. Conclusion - Next Steps\n",
        "\n",
        "**üéâ What We've Built:**\n",
        "\n",
        "A comprehensive **RAG system with multiple memory strategies**:\n",
        "- ‚úÖ Full conversation memory\n",
        "- ‚úÖ Windowed (sliding) memory  \n",
        "- ‚úÖ Summarization-based memory\n",
        "- ‚úÖ Vector-based semantic retrieval\n",
        "\n",
        "**üîÑ Combining Memory + RAG:**\n",
        "\n",
        "For production systems, you can integrate these components:\n",
        "```python\n",
        "1. Retrieve relevant facts from vector store (RAG)\n",
        "2. Load recent conversation history (Memory)\n",
        "3. Optionally include conversation summary (Summarization)\n",
        "4. Combine all context in prompt\n",
        "5. Generate informed response\n",
        "```\n",
        "\n",
        "**üöÄ Advanced Patterns:**\n",
        "- **Hybrid Memory**: Summary + recent window + vector retrieval\n",
        "- **Semantic Memory**: Store conversations as embeddings for cross-session retrieval\n",
        "- **Adaptive Windowing**: Dynamic window size based on conversation complexity\n",
        "- **Multi-Index RAG**: Separate vector stores for different knowledge domains\n",
        "\n",
        "**üìà Production Considerations:**\n",
        "- Use persistent storage (PostgreSQL, Redis, Pinecone)\n",
        "- Implement caching for embeddings\n",
        "- Monitor token usage and costs\n",
        "- Add conversation export/import functionality\n",
        "- Implement memory cleanup policies\n",
        "\n",
        "**Try Building:**\n",
        "- Personal AI assistant with long-term memory\n",
        "- Customer support bot with knowledge base\n",
        "- Research assistant with document retrieval\n",
        "- Code assistant with codebase context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNBQeibi3izb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}